# Encryption of sensitive data
# Context

CSB receives, stores, and passes credentials to terraform. For example, IaaS credentials,
database server credentials, binding account details, etc.
 
The credentials can be passed onto CSB in three different ways:
 - Environment variables
 - As parameters to a broker api request (e.g a cf create-service with a -c flag with options)
 - As a result (output) of a terraform operation (e.g admin password for a newly provisioned database, binding credentials, etc)

All these credentials are currently stored in the CSB database in plain text.

Most CSB tables contain credentials of some type:

* Table `service_instance_details` in field `other_details`.
    This field contains json including IaaS credentials, admin credentials for provisioned resources generated by terraform, and any other custom credential that the service offering defines in its yml file.
* Table `provision_request_details` in field `request_details`.
    This field contains json including all credentials passed in the provision request (-c flag). IaaS credentials, admin credentials for servers where we want resources to be created and any other custom credential that the service offering defines in its yml file.
* Table `service_binding_credentials` in field `other_details`.
    This field contains json including all credentials passed in the binding request and the terraform outputs generated during binding. IaaS credentials, admin credentials for servers where we want resources to be created and any other custom credential that the service offering defines in its yml file.
* Table `terraform_deployments` in field `workspace`.
    This field contains the terraform workspace including the terraform state file. The terraform state has all variables replaced, meaning that all credentials reside there in plain text.
 
We are addressing the need to encrypt this data.

## Key or password

1. The AES 256 algorithm that is the current standard requires a key.
We can allow users to specify a key directly.
If we do this, the key should be encoded in hexadecimal (or equivalent) otherwise character
encoding issues will lead to the use of low entropy keys.
1. Passwords. These can be converted into keys using an algorithm such as PDKDF2,
but it's necessary to add additional salt. This salt would have to be generated and then
stored with an association to the password, so that the same password results in the
same cryptographic key being used. An example to follow would be the Cloud Controller
which has a database table which associates password labels with salt.

## Key Management

1. They key could be managed via Ops Manager.
1. Environment variable set by the operator. It is riskier and operators may dislike it.
However is simpler at this point especially because there's a user base that have
abandoned the tile and do `cf push`.
1. We could autogenerate it at startup/first use etc. This would involve storing it in Runtime CredHub.

## Data access
Currently the field names in the database and property/variable names in code don't reflect that credentials are being stored. Also, 

1. Accessor Methods: 
    * Data is in some scenarios is accessed via accessor methods that do json marshalling/unmarshalling, and sometimes these bits are scattered through the code.
    * The CSB code base is consistent in the usage of each property, meaning either we always unmarshal it for use or always pass it around marshalled.
    * refactor to use always accessor methods for these properties and do encryption decryption there is simple
1. There is support from the ORM used (GORM) to create custom datatypes and it is straight forward. This approach is transparent and does not need much modification in the code. We would need to create 1 datatype that encrypts/decrypts (potencially 2 datatypes, if we want to push json marshal/unmarshal, as not all the fields are used in the same way)  

## Other considerations

There is the possibility to identify among the request data, outputs and environment variables, which ones contain sensitive information.
There are some well defined environment variables (like the ones that contain IaaS credentials) that could be encrypted separately. 
Support to indentifying which variables are sensitive could be added in the brokerpaks (e.g. with a `sensitive` flag added to them)

Advantage:
* Not all data is encrypted, better troubleshooting

Disadvantage:
* Needs modification of the brokerpak definition schema. 
* Needs identification of the fields, which can be different within each service offering and plan in each brokerpak. There's the possibility of some property falling off our radar or a later addion to fail to configure correctly
* It still doesn't solve all problems, this approach couldn't be applied to an incoming request before logging it, for example.    
   
## Out of scope

Logging of sensitive data is not in scope at the moment. Although we have done some exploration around it, there is more we need to discuss and get user feedback on. It will be tackled as a separate item.
Key rotation: will be looked at separately after this is implemented.

# Decision

We decided to encrypt the aforementioned fields. 

We will accept passwords, and will generate keys from the passwords. This is to be consistent with
the TAS tile (CredHub and CloudController configuration), and because we found that people got
confused when we tried to explain the approach using keys. This is principally because they
were familiar with public/private key pairs, but were generally not familiar with configuring
symmetric keys.

We will follow the password management approach used in the TAS Tile for CredHub and Cloud Controller.
This is:
- In the tile configuration, multiple passwords may be set. Each password should have an identifying
label, and may be marked "primary"
- To rotate a password, a new password is added and is marked "primary". The previous passwords are
not removed until the update has successfully been applied. This allows the broker an opportinity to
use the non-primary keys to read data, and the primary key to re-encrypt all the data.
- There is an internal database table that associates salt with each label. This allows the same
password (with the same label) to always result in the same AES256 key. Even when the broker database
has been restored from backup.

We will use `3. Accessor methods` to perform encryption/decryption. The reasons behind it are:
* It makes clear when and where credentials are being handled in plain text, as opposed of a custom datatype that would only be made visible in the model definition as an annotation.
* Decryption can be left out until credentials are actually needed and will be perform as soon as they are set, limiting the possibilities of leaking them with logging, under bad error handling etc. If we were to use the custom data type approach, encryption would happen as soon as we read from the database and delayed until we actually write to the database.
    
We will rename variables in the code to make clear which fields can contain sensitive data (as opposed to `other_details`)

We will consider renaming database fields.    

We will not look at the possibilities mentioned under [Other considerations](#other-considerations). Mostly because it is more complex, require more analysis at this point and could leak credentials if we somehow miss to identify a sensitive field in one of the services.    

The flow will be as follows
1. encryption key specified by in the tile, or via an environment variable.
2. All sensitive data will be encrypted/decrypted with that key within accessor methods

# Status
DRAFT

# Consequences

There is a risk that the operator will fail to provide the right encryption field and cause the loss of the data, and some data being encrypted with a different key. We can mitigate this by setting up some test values we can try to decrypt on start up, and fail to start if we can't. There is also the idea of continuing work on this and work on the solution to this mattters in a second iteration.

Accessing the data is not going to be as transparent as with a custom data type. However we think of this as an advantage in this context.

Troubleshooting becomes more challenging as all the parameters used are going to be encrypted in the database and eventually in logs.


 

